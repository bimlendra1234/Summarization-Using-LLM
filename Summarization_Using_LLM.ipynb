{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T2ZjD_xZZCXI",
    "outputId": "34c55823-21a1-4f0b-9b5e-a9f3b114ce14"
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets evaluate sacrebleu bert_score accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lEyvDZYoZOwM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    BartTokenizerFast,\n",
    "    BartForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwH1ZfSC6MVh"
   },
   "source": [
    "## Step 1: Dataset preparation and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 710,
     "referenced_widgets": [
      "f915ea1169774121beae136da78d87fb",
      "0fe1c135faca4239909d89fd156eb37b",
      "bc1f7884702f4e209936fb41f206ae48",
      "521754dec46a4b8ca252feadc78dce18",
      "c384f34cb0e14e0a9076d8966458c490",
      "58227ea421454b188410eb2b9045c180",
      "1573c32248834853acf3a6528732ac19",
      "9bdc71fb43044ab78f0e6ceb33d3bc61",
      "4d7be975c3c24ef1a25d5d9bd8f34c72",
      "1ef2168139cb4676afda37b443c8abeb",
      "df6db0cb020a48ab9167aacfba5668b4",
      "f76a3d73afa147aba669df8b9338907f",
      "f5a19b3c62d0478aa45f7134bb371639",
      "44ab7fff2de84e1e88585430deab38cf",
      "80f5e7a9cdd44190887d7123a9cbdd03",
      "6e395375d93940c688257b7db775eeb9",
      "b8d23ee5f00c43c48c344e161238bb3e",
      "49ce9eceda744c3eb5c250d02093d86c",
      "824a46684ce447e596c76f2bca14a40f",
      "f1c3cc167ba749de8aa644ea610011c6",
      "a104195e3d1f4f8ba399fe007082fa00",
      "0943bf119b80458a97602c3fff4fd5be",
      "631da8a2b5cb4568b298cb00ab9a0f4f",
      "a87732f095764f64b16fdde5ccaaa743",
      "5c877406586b4ab6b3be7d5e5dce5fd2",
      "94a0d4619de648139079005b0a69aef7",
      "2cb8dc08e63d435788be8b2f232c853a",
      "45f660ba4b7b4dee91ed340489d291a1",
      "958479ab6e2c484a93d95995425ed104",
      "9313310c1b6643a2bcc26100e8701b28",
      "08964544eeef4913921583bb99087933",
      "d54d83986be94a188e163f954bc57700",
      "0ae57be1f49346849842cb45790a491d",
      "94d5e28b451349f585261e63ad624de7",
      "47d057ba442a4da4b8e2d0e54670197f",
      "481b0aef145b49449746c1a56ee2be2c",
      "3f05e9499933436382bb3e76d8c69a35",
      "c5dca21b88d24dc28bea95a7b6810843",
      "edc98d7599b344319497db210a73901d",
      "fb012e9ea6fe4491a83e775c0a96418f",
      "4c78b7f1d5b14a3e9828c5433c5de311",
      "49ee685e19e648e1aa808a959a3533bd",
      "cbdc96c71514447b85c00cfc710ba2e8",
      "f206a6f1413b4d1f929ac07971f4f4c7",
      "e529dab69e124be1af82c4f18b146062",
      "267870c80f294311954e93965cff15b5",
      "cf7971823be04d3ba528e749a7eb15cf",
      "cf3c75da0c834e1c89872126de04dc6f",
      "7e08a3d434064c20a0f3224b837a7f7a",
      "ddf2f148c4b54f289995c900779040f9",
      "70408540f9d040c5a54623600371666f",
      "3247c5c8d1ea4a7699ecda52c6248438",
      "7c708b0e5f504b4b979ed7270b581267",
      "e9765e76b99f4873970b3bce62f2b3d0",
      "d71a8c095cfe46a4ad80d23814c6cb9b",
      "ece6e28bf6404842938ed9bdab34571f",
      "65a10656710742f58fcda33a982c778a",
      "b5ed5f87132e486e89bf0b5f2e9d36e5",
      "429a52c93af1406482e7de1f5be67e82",
      "fac8eaffc49845049cc9349bea0340b8",
      "71260f1025894d3ba0cdca8cc6904d69",
      "61c31f5b2a3a455d8df1feab64da011d",
      "5fdfd51e00df4ec99bdebf3bfaf42043",
      "e1fb3f7ca48b44369e1a082aaadf1525",
      "93219cc8b5e54ab8b184245cbe4eea36",
      "ebeea365e24e4500992ace9305c04d00",
      "a508d0e8c004403e8d34bf3c0c5ee2f2",
      "f873054b294a499dbbbb0d9b0e983a90",
      "5e7776fc112946c58c4a0f7ca9b9861a",
      "deee6e04a8654479b789c59ae63ef9ec",
      "19c5ae7c58ca41988d8e7a7c2691424a",
      "774765aeac5e469282ccb7e0df4b7c52",
      "a99537a9dba54cb1943f8b519028b169",
      "e9280802a8634c9e925b6d14618395fc",
      "c18e8358bd924ba582b4cd1274fbc048",
      "4140900458084205acf69e1cc4fb186c",
      "aec35920e36d47358f9aded31c88e479",
      "748671c36202443587a36910cd73c74e",
      "1a04c6d4c71c40f58c0ef4065eab1bc4",
      "ed0d71e632b54beda1b701c2be85c9dd",
      "6c3995028d0849f7b2fe0504276ab254",
      "e3db2da9d5b644bd800b654449a8c1fe",
      "515c92f9c4724f6992d065e722f22bfa",
      "2f373ff27ae44e9aad56e5915d025761",
      "d67b683df85640808d89975fc6929317",
      "648846d263a3423da05052836250bf13",
      "dcb757748b2944489a3cd7dec0d9eaa8",
      "c23009b5732c449db54b64fb171b8aa2",
      "7afa1111340b47478db135212d37917a",
      "7cd42fa3027f4ccba2e3c76fd5972ec5",
      "35a794927ddd4692933fc0cc4968d442",
      "58b3447fb92a49a8b47b5af9025a4c79",
      "f5b1363c2ea04791bf58f58487fa9dbf",
      "b9851cbaf5b042a3a00aa5b2deed2570",
      "4325e8c406a7496186d6b24a959b1178",
      "28d5781967414b2996e5c669dca62ad4",
      "f7590c74f1af49cbb833b5fe2f98f543",
      "4b3aa7285a764acba816acbd2feccf11",
      "544b94dbd64c4d138372c11cc33f784a",
      "ddb2e3a1f4db44c2b6347c53ace79367",
      "ccc9a49b4b5843e5afd6ec4eb78fb766",
      "2e41251e94be4d77a326699cdfd6fae6",
      "3fcb54f15efb4da0b8e000ed9aa2b399",
      "d8d11ced9dd24d748360cb18a9c19c1a",
      "5b4e15edaae649c0bcbd50e33f15cfe7",
      "f2e141d093af4939af857ded3cb01731",
      "1dd05093d41b451fbe408f0d4ad4dc7f",
      "d569564b6fea46cda6517012a579d96d",
      "69fb361938654ca7af198f8237fcdb17",
      "b72d2bd44c4e4a2694c6abb5c9262e5c",
      "31b3e7d245ba408db204623d574b66b5",
      "9e6be43966424bc7ba3050cc801ef8fe",
      "ffd6f0b03de7401bb631901aa681c7c5",
      "cc5140b4e4a44caab36ca6927e338ab8",
      "1135651e5b87446dbd60f3c39bb150a7",
      "7292ac0b48924f4980c9b68f35b16bc0",
      "34520695e89f4ea7bc91b9f9c7567894",
      "74c976fcd63e4abead22e8e75442a9c0",
      "37d6850e4ec64a29abb2221825549478",
      "2a8288da7d134624884500dddb481c2f",
      "53173d885a3b445cb3858711267507a8"
     ]
    },
    "id": "vfn01z4mZYjC",
    "outputId": "601137f4-1421-4fa8-9368-bd77e8a3b230"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"multi_news\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_nOgJepB2yJD",
    "outputId": "7dff099f-5fc1-421d-e464-9b567da2867e"
   },
   "outputs": [],
   "source": [
    "def datasetOverview(dataset):\n",
    "    train_size = len(dataset[\"train\"])\n",
    "    val_size = len(dataset[\"validation\"])\n",
    "    test_size = len(dataset[\"test\"])\n",
    "\n",
    "    print(f\"Training Samples:  {train_size}\")\n",
    "    print(f\"Validation Samples: {val_size}\")\n",
    "    print(f\"Test Samples:      {test_size}\")\n",
    "\n",
    "    print(\"Sample Entry from Training Set:\")\n",
    "    print(dataset[\"train\"][0])\n",
    "\n",
    "def analyzeTrainingLengths(dataset):\n",
    "    train_docs = dataset[\"train\"][\"document\"]\n",
    "    train_summaries = dataset[\"train\"][\"summary\"]\n",
    "\n",
    "    doc_word_counts = [len(text.split()) for text in train_docs]\n",
    "    summary_word_counts = [len(text.split()) for text in train_summaries]\n",
    "\n",
    "    avg_doc_len = np.mean(doc_word_counts)\n",
    "    avg_summary_len = np.mean(summary_word_counts)\n",
    "\n",
    "    print(\"Average document length (train):\", round(avg_doc_len, 2))\n",
    "    print(\"Average summary length (train):\", round(avg_summary_len, 2))\n",
    "\n",
    "\n",
    "def analyzeValidationLengths(dataset):\n",
    "    val_docs = dataset[\"validation\"][\"document\"]\n",
    "    val_summaries = dataset[\"validation\"][\"summary\"]\n",
    "\n",
    "    doc_word_counts = [len(text.split()) for text in val_docs]\n",
    "    summary_word_counts = [len(text.split()) for text in val_summaries]\n",
    "\n",
    "    avg_doc_len = np.mean(doc_word_counts)\n",
    "    avg_summary_len = np.mean(summary_word_counts)\n",
    "\n",
    "    print(\"Average document length (val):\", round(avg_doc_len, 2))\n",
    "    print(\"Average summary length (val):\", round(avg_summary_len, 2))\n",
    "\n",
    "\n",
    "print(\"dataset sizes and a sample: \")\n",
    "datasetOverview(dataset)\n",
    "\n",
    "print(\"\\n Average lengths in training data: \")\n",
    "analyzeTrainingLengths(dataset)\n",
    "\n",
    "print(\"\\n Average lengths in validation data: \")\n",
    "analyzeValidationLengths(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nnKDLqq6Zv2"
   },
   "source": [
    "3. Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337,
     "referenced_widgets": [
      "6389ec77bce84fb3a3850900882c8227",
      "1ad18f32d4204dd681f7b4a375ec4728",
      "b0c217baf78e46da8c5a15378b3706c3",
      "6c23b1166e0e4b89ae37fbd856be4ec5",
      "cb421490f5544da7ad5a588d33f34d0c",
      "fb76bb79dc4a464ea39f28389341f6a3",
      "a0554d109472456995a7322e674b7e4f",
      "1df376aadbdb42e88f113cfb4f6c22fe",
      "ee76817f2936406e9f947e2b4bb688d1",
      "9557440599824cbf83144d21e0e59802",
      "6e2b0b96d2044ad49dd093444e2892d9",
      "c762cd6b7ac74cf8be30deb6fde3f9fc",
      "5d87cf0c849a420fa80529a529d1101e",
      "cc4e67712784439c9760e4fca330c38b",
      "72f2125e7da9418ab59e01d68222932a",
      "ac58763b1a424799a35a05e03260728d",
      "c63f7c74a1784dfea86c0eac50305618",
      "22c6674a1b474a1fbf8046de3486a69e",
      "f96363013c614fe5a0543b6953761277",
      "7d2fe6c98ff9489bbacb4c1d4bcadeb6",
      "0ea0ce9c65654c90810a55eb192a6429",
      "51fe54eca76443b98f9a5de6f6c21b53",
      "08814f983bd04a83a95db6987552c54c",
      "fbaa7452dc5b4ac686496805182e0b5f",
      "23ddcac5f3954c2dac0d6634f8705a51",
      "e65b6c59b04943339bbd086910b6d184",
      "5f395ce88730412fa3854e11be916c7a",
      "19a9312c9c1f44ed8336063fcbfab3e7",
      "626ebf389d4641628fbba9914f3aec32",
      "9f20f14a63fa452fb7c49bcd60084bcb",
      "f295a035af1c49ba921b3759bd755247",
      "e5faeaea9ef44d8dae3d780fcf278268",
      "c525e14525464f30ab3d2ad3aa247bd5",
      "a2fe45c15295449998f6d99c6d687b02",
      "6260aca926bf4570b49c2d4117547674",
      "573f055139bd4ad589c0544bc7aab5a4",
      "b808cfd974a6446c80c38380a7031157",
      "fca2c5221df24f51b05f94bada0425fc",
      "09a8496606094e5cbe5cb5e7559db988",
      "e355b67d2d8a402c9cba5c61d74acf34",
      "757b7f142ec048109a5202a6405d66df",
      "7c68f5a526f7475883ee0a37356e6568",
      "b73a7639caa642379120e700ac0f9b4a",
      "66205e8fb8be4789b8c9b02fd0584b53",
      "2788a1a51bcd442a8760e1bc753ce1b6",
      "195b6d8492cd4a22b73e427ce7abbced",
      "4eb9a2b983ca4961babb4e348d167f22",
      "ef711703e1e2447bba3b0f27d2dcd3b7",
      "1f9eb736238344e59c0101098dd38bbd",
      "65f7d6ca1d3e42468aa1882fde6ea9d2",
      "df288b107bdd494d9399168f57e3ec62",
      "83c1a2a9370b49999eec641fe222dee5",
      "4b27659dae2943e8914573a1cb9f6bd2",
      "b10a1d1028ef4d9d81363f5b3d9a0ad5",
      "8674598751fa4931a157a4ea2f05bbe5",
      "8a38db9bb0b44e34999a432388baf04a",
      "7f12fe9690754c198be949fadddc4d18",
      "0986a1ee6e0e4013a2499b25fea61719",
      "b3d9278499bb42ca8aa141fb95359b9c",
      "d27c8e7acfdd4067855d8dc1ee4cdeea",
      "3019ded65456493c93d5f42046d12915",
      "5907a86e2f634b35997efbffe9c5199c",
      "eca82ee884a84be4840ec7bf9f19b440",
      "6b5d375485ff4c0da18d04c435b268a4",
      "94de5b2205274f94b01154b418160083",
      "dbde3b5267364f569b8719434e53c8a1",
      "b45acc38d937470ca93a128a5e7ed7be",
      "5f899edeedab4db7a3474a122a4fb140",
      "c72154a8e0d04b99b317256444d93b43",
      "e4d5cb4ce7da412ab9214ad9bca249ed",
      "f56a2b1510264f368d9e84c123c460fc",
      "de9aaa8a15454feaa8491a262c3b34b9",
      "7edf12cc913e4971813e151cafa67386",
      "8daef32fc272486ab5a293bd61a4c7ec",
      "b83fe04f63814b479723d384d0e5d237",
      "6b40343e868b4daca96ee708287fd227",
      "c410cef8eed94670a6c300b09793766f",
      "a4f39580fdb34cd699bd3fb5dc976464",
      "9a866c23b8c24ac2a14b3210e1d2aaa1",
      "361dc9da3ae842d19c80480e59e1951b",
      "48da1dcac18d4b2f9e2a73ec111cd4a9",
      "a83fd1b5270d449393d4f8c794ee2e55",
      "f0fdb3468d1146ffb1ae40d43fb1910e",
      "823861823f444d4883c13547193b04e2",
      "64962455abf64c49ac86e5e3d2aa24c8",
      "0e15a951b1694415b2bc882ede635daa",
      "9b859ff01bf3451688d6c0535cf9b8cb",
      "b03975a3fe2c43aa88f5224e6989ee9d",
      "237f7d074db14503a40d7b1507ab6173",
      "b8c888b080e14c819d032aa84de1e55e",
      "cbbd7f73012146799aa3621f3e8c52e9",
      "334f9c871e72477da8de7a228b9b1b83",
      "e1035ce5ea1040b39cb070aac22b8cd0",
      "bb60ada885a84546b042692cad98aa81",
      "c95fa270478e457cb8d88cd772094dfa",
      "fb73902d99b64669978e643556a7c171",
      "37c4ed3adad4442890ff9d0df0b48268",
      "d4258c0f88ec48339bc3a575d271bf71",
      "350b2fa9033b43fdad39a3ebf3e3923f",
      "316b21cd56ff42209d9cb26eae0db68d",
      "6c7576c05f524ca89d6febfbdec3ee61",
      "71da39beacfa4cbb84639fdc6b5ce36c",
      "42eef51d296d4120a1fba9f6264fa219",
      "d8b02bcaf41742aa8c2eb958d001f999",
      "d91b0563bec542a0a41a12d70d1b5bb9",
      "43d2ffa9bdd04428a753c46df76e80a3",
      "b8069291220e4bf98b41e102bf5e52dc",
      "f2041d9b51d3499184897d578c89072b",
      "87c1dd61685c40a08c349d762302131a",
      "348001048d0147238daf0df1ff5ea1f9"
     ]
    },
    "id": "_7uKWQBCZpr8",
    "outputId": "1fa7d126-ef40-411c-fea8-62eafc5af2cb"
   },
   "outputs": [],
   "source": [
    "# Loading tokenizer\n",
    "baseModel = \"facebook/bart-base\"\n",
    "bartTokenizer = BartTokenizerFast.from_pretrained(baseModel)\n",
    "\n",
    "# maximum length for inputs and targets\n",
    "sourceMaxLength = 1024\n",
    "targetMaxLength = 256\n",
    "\n",
    "def dataPreparationPreprosessing(batch):\n",
    "    # Tokenize both inputs and targets\n",
    "    # Articles are padded and truncated to 1024.\n",
    "    # Summaries are padded and truncated to 256.\n",
    "\n",
    "    sourceText = batch[\"document\"]\n",
    "    targetText = batch[\"summary\"]\n",
    "\n",
    "    tokenizedInput = bartTokenizer(\n",
    "        sourceText,\n",
    "        max_length=sourceMaxLength,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    with bartTokenizer.as_target_tokenizer():\n",
    "        tokenizedTarget = bartTokenizer(\n",
    "            targetText,\n",
    "            max_length=targetMaxLength,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "    # Replacing padding tokens in target with -100\n",
    "    updatedLabel = [\n",
    "        [-100 if token == bartTokenizer.pad_token_id else token for token in seq]\n",
    "        for seq in tokenizedTarget\n",
    "    ]\n",
    "\n",
    "    tokenizedInput[\"labels\"] = updatedLabel\n",
    "    return tokenizedInput\n",
    "\n",
    "# Mapping the preprocessing function to each dataset split\n",
    "tokenizedTrainData = dataset[\"train\"].map(dataPreparationPreprosessing, batched=True, remove_columns=[\"document\", \"summary\"])\n",
    "tokenizedValData   = dataset[\"validation\"].map(dataPreparationPreprosessing, batched=True, remove_columns=[\"document\", \"summary\"])\n",
    "tokenizedTestData  = dataset[\"test\"].map(dataPreparationPreprosessing, batched=True, remove_columns=[\"document\", \"summary\"])\n",
    "\n",
    "\n",
    "tokenizedTrainData.save_to_disk(\"multi_news_tokenized_train\")\n",
    "tokenizedValData.save_to_disk(\"multi_news_tokenized_val\")\n",
    "tokenizedTestData.save_to_disk(\"multi_news_tokenized_test\")\n",
    "\n",
    "\n",
    "paddingColator = DataCollatorForSeq2Seq(tokenizer=bartTokenizer, model=baseModel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bPUf11H6_Eo"
   },
   "source": [
    "preprocessing methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMwH1HkZ7EH0"
   },
   "source": [
    "\n",
    "In the preprocessing step I  implemented tokenization of input document and their respective summaries using the BartTokenizerFast from the facebook/bart-base model using Hugging Face's map() function with batching enabled for efficiency. Each document was truncated and padded to a maximum of 1024 tokens with summaries truncated and padded to 256 tokens max. Truncating documents prevents long input text from breaking the model length limit, while padding documents ensures that all input in a given batch have the same input shape for appropriate batching. In the case of summarizing tokenization padding tokens were replaced by -100 to ensure these tokens were ignored for calculating the loss. I also wrapped this tokenizer with a DataCollatorForSeq2Seq to dynamically pad specific training data. Lastly I saved all pre-processed datasets locally so I do not have to reprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPXErtff7JP7"
   },
   "source": [
    "## Step 2: Model Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NeFVf6Gk7NIH"
   },
   "source": [
    "Useing the pre-trained model `facebook/bart-base` from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "b41e97153f104db085a507228fecd6d2",
      "1490030f50da423b8b052cdca83adbc6",
      "e9dc85d93d464a36894d1d54cb3e5360",
      "d895daeff3834c398efac4db6f2effd8",
      "518a253b66ea4cc6b2c2d0eee06a27cd",
      "c612c100daa948039be71ecf8ec1dcdc",
      "7f761ba0067d4d5f94d417ba8a81cc03",
      "47585fe88c7e427384f3aad7e9bb8271",
      "1d1dd734f08c4b9bb3533d480e7ea554",
      "f93e16bebecc41eeb380ccd126674daf",
      "2d57fdcf169a481c8bc8a42219f0a860"
     ]
    },
    "id": "kwoBFm1KxhHl",
    "outputId": "223d457e-3a2d-4eb1-e3cc-e8f20e757600"
   },
   "outputs": [],
   "source": [
    "# Defining BART Model\n",
    "model = BartForConditionalGeneration.from_pretrained(baseModel)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3nnWJFaJjvas",
    "outputId": "7297941a-debc-4635-81ee-a238bd37376e"
   },
   "outputs": [],
   "source": [
    "pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "9a17aaa055224a899ca67bb40b05f207",
      "53a3819d9a9742598d6e3a562e7b79da",
      "04c5db6402de40b8bc8c47ccfcc73cf6",
      "442b9f76acd34f2dadd941434fc7cf64",
      "831db787cbf242758d4cf96954c8dfdf",
      "940bc67578df46bd9672dfee9be9d334",
      "587a7e0238ec4c01b5614e89e489b61c",
      "086728c4a2114e9d859e476d47babc08",
      "be63f65c6f984cfe819e04754b166883",
      "2011abfa3a954742b0ffe00a2d879b8e",
      "5a249e5c59f7404ea15eff0fc606bdc7",
      "b74e1bfc92944a52a79eadf1d753e9ea",
      "dbdcdd1085dc468a9ac525262caf948e",
      "70291db95ba4433bbbc5a338da2ac434",
      "ee13876f23a3434d8bfdc9d9c81966d0",
      "6dec82be654c4278a01fda4ae37f0373",
      "7980fad71f744175b0fdc856b91dda86",
      "17e2a94261bf4c7db7ef952e02c35951",
      "2bdb5671d1784fa1bfa607c8ce9b08f7",
      "c08898b61bd34f29843e736995902626",
      "890ad502a9e940da9c1622b72f7dcc52",
      "be298fcb7e984baba1fed7e219276f6b",
      "e7174004f8a04479be2b4a545bd33cd8",
      "83dab7e9c90a4c1facd3245db74acbfb",
      "c5d9d636b1464203b7dac9fa6ccc5514",
      "0ce7bc464fc04cea93e657c7aed80c2f",
      "63eef1edc7de44629869ca138b6b7966",
      "b866e62558954d138f309e50ee01372a",
      "90310cf4d8494d71ad55f69c58514c97",
      "c43f972a9ee34291b1122f8664b26bde",
      "9f1be581e0414d4c8709735586fa671a",
      "74e6bc1fedc9433e9871a509b89d35ca",
      "cb39976602e04ac599c3d88e094d6278"
     ]
    },
    "id": "7ycdEp3l9lIp",
    "outputId": "9fc61a57-b43d-421b-959c-20ca92a9a12e"
   },
   "outputs": [],
   "source": [
    "# Defining and loading metrics ROUGE, BLEU, BERTScore\n",
    "metricRouge = evaluate.load(\"rouge\")\n",
    "metricBleu  = evaluate.load(\"sacrebleu\")\n",
    "metricBert  = evaluate.load(\"bertscore\")\n",
    "\n",
    "def decodeOutput(pred_ids, tokenizer):\n",
    "    if isinstance(pred_ids, torch.Tensor):\n",
    "        pred_ids = pred_ids.detach().cpu().numpy()\n",
    "    pred_ids = np.clip(pred_ids.astype(np.int64), 0, tokenizer.vocab_size - 1)\n",
    "    return tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "\n",
    "def evaluateComputeMetrics(eval_predictions):\n",
    "    generated, reference = eval_predictions\n",
    "\n",
    "    # Decoding predicted sequences\n",
    "    generatedText = decodeOutput(generated, bartTokenizer)\n",
    "\n",
    "    # Decoding reference labels while ignoring special padding value -100\n",
    "    referenceText = []\n",
    "    for target in reference:\n",
    "        filtered_ids = [token_id for token_id in target if token_id != -100]\n",
    "        referenceText.append(bartTokenizer.decode(filtered_ids, skip_special_tokens=True))\n",
    "\n",
    "    # Computing ROUGE score\n",
    "    rougeResult = metricRouge.compute(predictions=generatedText, references=referenceText)\n",
    "    scoreRouge1 = round(rougeResult[\"rouge1\"] * 100, 2)\n",
    "    scoreRouge2 = round(rougeResult[\"rouge2\"] * 100, 2)\n",
    "    scoreRougeL = round(rougeResult[\"rougeL\"] * 100, 2)\n",
    "\n",
    "    # Computing BLEU score\n",
    "    bleuResult = metricBleu.compute(\n",
    "        predictions=generatedText,\n",
    "        references=[[ref] for ref in referenceText]\n",
    "    )\n",
    "    scoreBleu = round(bleuResult[\"score\"], 2)\n",
    "\n",
    "    # Computing BERTScore\n",
    "    bertResult = metricBert.compute(\n",
    "        predictions=generatedText,\n",
    "        references=referenceText,\n",
    "        lang=\"en\"\n",
    "    )\n",
    "    scoreBert_f1 = round(float(np.mean(bertResult[\"f1\"])) * 100, 2)\n",
    "\n",
    "    return {\n",
    "        \"rouge1\": scoreRouge1,\n",
    "        \"rouge2\": scoreRouge2,\n",
    "        \"rougeL\": scoreRougeL,\n",
    "        \"bleu\": scoreBleu,\n",
    "        \"bert_f1\": scoreBert_f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JdDqQt17WJ9"
   },
   "source": [
    "2. Training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BISINyZx7cm_"
   },
   "source": [
    "   - Fine-tune the model on the tokenized training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lz7AwkTf1sEl"
   },
   "outputs": [],
   "source": [
    "# Setting up training arguments\n",
    "def configureTrainArgs(output_path=\"bart-multinews-checkpoints\"):\n",
    "    return Seq2SeqTrainingArguments(\n",
    "        output_dir=output_path,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        logging_steps=500,\n",
    "        learning_rate=3e-5,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=5,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=256,\n",
    "        push_to_hub=False\n",
    "    )\n",
    "\n",
    "# Initializing custom Trainer\n",
    "def setupTrainer(model, tokenizer, train_data, val_data, data_collator, compute_metrics_fn, args):\n",
    "    trainerInstance = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics_fn\n",
    "    )\n",
    "    return trainerInstance\n",
    "\n",
    "\n",
    "# Creating training args\n",
    "args = configureTrainArgs()\n",
    "\n",
    "# Creating a smaller subset of train data and val data and used 1000 samples from the training set and 100 from the validation set.\n",
    "subsetTrain = tokenizedTrainData.select(range(1000))\n",
    "subsetVal = tokenizedValData.select(range(100))\n",
    "\n",
    "# Initializing the trainer\n",
    "trainer = setupTrainer(\n",
    "    model=model,\n",
    "    tokenizer=bartTokenizer,\n",
    "    train_data=subsetTrain,\n",
    "    val_data=subsetVal,\n",
    "    data_collator=paddingColator,\n",
    "    compute_metrics_fn=evaluateComputeMetrics,\n",
    "    args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9Uy0Fwk7pPl"
   },
   "source": [
    "   - Monitor training and validation loss over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 605,
     "referenced_widgets": [
      "28e4be8694954bbeb183d0e0d6459986",
      "bce82c31f2fc421b86430afd21b5e1c0",
      "37c2dcdbedba4d0980581bbbab3314bc",
      "6df23163f7e54e05926aa9773f39e4e7",
      "75c492db7b10453fb64ce9c1f59d4e33",
      "a7709efc38814145b86661282b5f9260",
      "1f3513efdbd74199af93498dd7f68a6c",
      "69f345b45db24bd0928375b757f9c9ad",
      "fd368c2ca8c94001a380b95cd44c0d29",
      "2f76e9bc772946c7967a467d2ece17e7",
      "f1b69d1021cf41e1b35e611e98d41af3",
      "f2365f7fcd5e4052bd81f40d4e65c8aa",
      "54bd4f8ab3104401b168691d24715411",
      "d3b937f0bf1a4936a486770a4a127af1",
      "460d3d6f344b439590447ed61720f268",
      "ec5dbcb7f5324e7ba229646c27a81342",
      "205bf002d26b4db7a530ed7fba520743",
      "9db0f273c1c341858d50aba983708ae5",
      "4d90daa697d3491eb61845d4c8a8b234",
      "5e5706b483234c4c9d208d8a1154cf67",
      "8fe7a922cdc245b3a71cff5f6e944659",
      "91d4fd886f3d457f8ab7d78adf3d144b",
      "e003fa9970e243ad922fd86c7bfd54eb",
      "b202aba5753f494987e02d13849c36ba",
      "055c5ed191f74880bc3172ddce39364e",
      "7452ce5153024c92bd7c89017219c5ce",
      "325b5a7a3dc44d8e88e2722da9a1fbdc",
      "e9f01345dd12463590be97c3607f6b08",
      "a951859699644c8681e2b5bc610dbc31",
      "7e9d6b62bf7d42f899b04dbfb67fe9d8",
      "3ba5ed9e2ec949d6a22ff657583b4182",
      "e675c272366d4aaabf7137f7529ed5bd",
      "17487bbc7f0b4203b3da6924c2b590c1",
      "c2137a505c7246949611a1b0711fcb46",
      "bae36c81c0c7402e8453da246d523459",
      "05ff118d8dc34a699d1562571dedbe39",
      "1a4e68ede9bb4566bf746e44ad439ec0",
      "717093ea1e5940f4a66d495feb2efb6c",
      "a2c916b5239641838c787598263e22da",
      "b0691bc7d20e4a05b9d450541bc22fee",
      "0c6b7519818c470293424c66579be513",
      "82cdf7bc04e24a61ad1af09fc3ece791",
      "eeb5783c420d4032b6d46b081ac9e2e5",
      "f02b24a15ec94dd9b7ffd3e2fa571671",
      "243da02a6af34795ae5507d834fdf73c",
      "01ab6402689140bb8e13818c94781e85",
      "7ef51624d2214c7da198204aefd333ff",
      "f4b4b877b1aa4f748eb4d6ac73b7f25b",
      "bc1d70d29260442894c3517ec8895a57",
      "942f251d5cc148e3901550063e64ca96",
      "c1afe0d3225341f6a182280c612adf47",
      "c41f4c5d1a524be0ab92099df5602fb9",
      "d6c06d07935a41a39c92f1512dcd297f",
      "b26593206c0247c18a68ed18e5b1e290",
      "f553e4548e064f90b2be2efb237bb0c7",
      "bff8c1d7d4204fde85e92b8a95afdc1f",
      "97f898ba0cd14585b485fd11574dccd5",
      "82b26031727c47f0822f88cb22a1a4b5",
      "941687bd41394911b45ca0a032b3b6d1",
      "97d7334e05ef4ec9968690735ba3bb19",
      "f9c7927a1b294529accd3ac1a7212526",
      "37e90b5483794ceaad8bd03d784980b1",
      "6946f39697d54729ae0dae4922092a1a",
      "6ce851bb3e824f38af999a5c7834009a",
      "29094ba2149642b59351a00f22b204da",
      "18bebe6db2814931bb864245e0ad3768"
     ]
    },
    "id": "ltPs1J-gJU60",
    "outputId": "e6034b8f-abc9-40bd-d7a8-b50dbd5cf903"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "trainOutput = trainer.train()\n",
    "\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", trainOutput.metrics)\n",
    "trainer.save_metrics(\"train\", trainOutput.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9Mh2-jI8fsM"
   },
   "source": [
    "training methodology (e.g., hyperparameters used, training process, and any challenges faced)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1z0PHF628j7i"
   },
   "source": [
    "\n",
    "To train the model I fine-tuned the facebook/bart-base model using 1000 training sample and 100 validation sample from the Multi-News dataset. A learning rate of 3e-5 was used with batch size 1 with 4 step of gradient accumulation and giving an effective batch size of 4. The model was trained for 5 epoch. The model used mixed precision (i.e. fp16) to speed up and reduce memory usage during training. The Seq2SeqTrainer was used with the evaluation_strategy=\"epoch\" option to examine validation performance after each epoch and predict_with_generate=True to evaluate using the model generated summaries. The training loss and evaluation metrics were logged and visualized over each epoch to track performance throughout training.\n",
    "\n",
    "\n",
    "One of the main challenges was managing GPU memory limits, which I addressed by reducing batch size and using gradient accumulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6yKovoQ8tVZ"
   },
   "source": [
    "## Step 3: Evaluation and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "VMBoZ2y4dd2h",
    "outputId": "5ff4b1c7-9964-4d01-bccc-4379a437fda7"
   },
   "outputs": [],
   "source": [
    "# Plotting training and validation Loss over epochs\n",
    "def plotTrainerLossCurves(trainer):\n",
    "    training_epochs, training_loss_vals = [], []\n",
    "    validation_epochs, validation_loss_vals = [], []\n",
    "\n",
    "    for record in trainer.state.log_history:\n",
    "        if record.get(\"loss\") is not None and record.get(\"epoch\") is not None:\n",
    "            training_loss_vals.append(record[\"loss\"])\n",
    "            training_epochs.append(record[\"epoch\"])\n",
    "        if record.get(\"eval_loss\") is not None and record.get(\"epoch\") is not None:\n",
    "            validation_loss_vals.append(record[\"eval_loss\"])\n",
    "            validation_epochs.append(record[\"epoch\"])\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(training_epochs, training_loss_vals, label=\"Training Loss\", marker='o')\n",
    "    plt.plot(validation_epochs, validation_loss_vals, label=\"Validation Loss\", marker='o')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training vs Validation Loss over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plotTrainerLossCurves(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "QbATV4UYlKdo",
    "outputId": "9723f794-7c0d-408d-a471-f63151d5bd8b"
   },
   "outputs": [],
   "source": [
    "# Plotting training and validation Loss over logging steps\n",
    "def stepwiseLossProgress(trainer):\n",
    "    recorded_train_losses = []\n",
    "    recorded_val_losses = []\n",
    "\n",
    "    for entry in trainer.state.log_history:\n",
    "        if \"loss\" in entry:\n",
    "            recorded_train_losses.append(entry[\"loss\"])\n",
    "        if \"eval_loss\" in entry:\n",
    "            recorded_val_losses.append(entry[\"eval_loss\"])\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(recorded_train_losses, label=\"Train Loss\")\n",
    "    plt.plot(recorded_val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Logging Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Validation Loss across Steps\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "stepwiseLossProgress(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "id": "z28vj9rwdi8x",
    "outputId": "12dc4243-141f-4eb6-de89-0d9e750c6361"
   },
   "outputs": [],
   "source": [
    "# Plotting separate metrics ROUGE, BLEU, BERT over epochs\n",
    "def plotEvaluationMetrics(trainer):\n",
    "    epochs, rouge_1, rouge_2, rouge_L = [], [], [], []\n",
    "    bleu_scores, bert_f1_scores = [], []\n",
    "\n",
    "    for entry in trainer.state.log_history:\n",
    "        if \"eval_rouge1\" in entry:\n",
    "            epochs.append(entry[\"epoch\"])\n",
    "            rouge_1.append(entry[\"eval_rouge1\"])\n",
    "            rouge_2.append(entry[\"eval_rouge2\"])\n",
    "            rouge_L.append(entry[\"eval_rougeL\"])\n",
    "            bleu_scores.append(entry[\"eval_bleu\"])\n",
    "            bert_f1_scores.append(entry[\"eval_bert_f1\"])\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    metric_titles = [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"BLEU\", \"BERTScore\"]\n",
    "    metric_data = [rouge_1, rouge_2, rouge_L, bleu_scores, bert_f1_scores]\n",
    "\n",
    "    for idx, (title, values) in enumerate(zip(metric_titles, metric_data), start=1):\n",
    "        plt.subplot(2, 3, idx)\n",
    "        plt.plot(epochs, values, marker='o')\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plotEvaluationMetrics(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 271
    },
    "id": "Fli7DZh7LYkM",
    "outputId": "ac204492-1cff-4c28-86f3-8aa77f2ea22b"
   },
   "outputs": [],
   "source": [
    "# Evaluating on test data\n",
    "def evaluateOnTestSet(trainer, tokenized_data, sample_size=500):\n",
    "    subset = tokenized_data.select(range(sample_size))\n",
    "    results = trainer.evaluate(eval_dataset=subset)\n",
    "    print(\"Evaluation Results on Test Set:\", results)\n",
    "    trainer.log_metrics(\"test\", results)\n",
    "    trainer.save_metrics(\"test\", results)\n",
    "    return results\n",
    "\n",
    "TestMetricesResult = evaluateOnTestSet(trainer, tokenizedTestData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "EFQJOOBNlgIL",
    "outputId": "d5616c2a-cb82-4d22-a0f8-0e7c6543c002"
   },
   "outputs": [],
   "source": [
    "# Bar chart of final test metrics\n",
    "def testMetricBarPlot(test_metrics):\n",
    "    metric_labels = [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"BLEU\", \"BERTScore\"]\n",
    "    metric_keys = [\"eval_rouge1\", \"eval_rouge2\", \"eval_rougeL\", \"eval_bleu\", \"eval_bert_f1\"]\n",
    "    scores = [test_metrics[key] for key in metric_keys]\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    bars = plt.bar(metric_labels, scores, color=\"skyblue\")\n",
    "\n",
    "    for bar, value in zip(bars, scores):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, height + 0.5, f\"{value:.2f}\",\n",
    "                 ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Evaluation Metrics on Test Subset\")\n",
    "    plt.ylim(0, max(scores) + 5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "testMetricBarPlot(TestMetricesResult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfu5uGThzCVX"
   },
   "source": [
    "detailed analysis of the model’s performance for each evaluation metric.\n",
    "\n",
    "The model performed well on all the evaluation metrics. The ROUGE-1 score of 41.87 and ROUGE-2 scores of 14.72 tells that the summaries have included many important words and phrase from the original article. The ROUGE-L score\n",
    " of 21.65 indicate that the model maintain the structure and order of content reasonably well. The BLEU score of 11.81 shows that the generated summary exhibit a congruency with the reference summaries in terms of word pattern and grammar. The BERTScore 86.16 was high which means that the summaries included similar meaning to the main article even with different words. Therefore the score confidently establish that the model is generating summaries that include meaning and are very accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGl7tupH9NHv"
   },
   "source": [
    "challenges faced during evaluation (e.g., handling long documents, variability in summary quality, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4rcvZbF9QF3"
   },
   "source": [
    "\n",
    "One of the significant during evaluation was working with long input document as a number of article in the Multi-News dataset exceeded the 1024 token BART model limit.  we had to truncate the input, which may lead to the omission of critical context and the potential for reduced summary quality. Additionally variability in human written summaries, in terms of both style and length, meant that the model didn't always neatly correlate with reference summaries. Finally computing evaluation metrics like BERTScore was tedious because this involved contextual embeddings. Limited GPU memory also imposed limits on how many test samples we could evaluate at once that is why the evaluations were subsequently conducted on smaller chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3v8y_rZ9XRu"
   },
   "source": [
    "Propose potential modifications or extensions to enhance summarization quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysmFgonV9a2o"
   },
   "source": [
    "\n",
    "To improve summarization quality, we can use a larger model like facebook/bart-large for better understanding of longer texts. We can also try models made for long documents, like Longformer or LED. Increasing the input length limit beyond 1024 tokens can help include more context. Using more training data or fine-tuning for more epochs may also improve performance. Adding techniques like coverage loss or reinforcement learning could reduce repetition and improve summary quality. Finally, post-processing summaries to clean or shorten them can make the outputs more readable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ztn-iKjG9eJU"
   },
   "source": [
    "References."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZgpuBWg9isM"
   },
   "source": [
    "\n",
    "1. https://huggingface.co/docs/transformers/index\n",
    "2. https://huggingface.co/facebook/bart-base\n",
    "3. https://huggingface.co/datasets/multi_news\n",
    "4. https://huggingface.co/docs/evaluate\n",
    "5. https://github.com/huggingface/transformers/blob/main/examples/pytorch/seq2seq/README.md\n",
    "6. https://huggingface.co/blog\n",
    "7. https://matplotlib.org/stable/index.html"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
